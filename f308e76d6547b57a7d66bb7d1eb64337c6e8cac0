{
  "comments": [
    {
      "key": {
        "uuid": "bf51134e_4ad8f11c",
        "filename": "/COMMIT_MSG",
        "patchSetId": 4
      },
      "lineNbr": 21,
      "author": {
        "id": 13252
      },
      "writtenOn": "2020-06-24T06:33:01Z",
      "side": 1,
      "message": "Where is this going to be used? Is there some glance or nova job that depends on this to actually exercise this option? If yes, please add a reference as \"Needed-By:\"",
      "revId": "f308e76d6547b57a7d66bb7d1eb64337c6e8cac0",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "bf51134e_b6c92fad",
        "filename": "/COMMIT_MSG",
        "patchSetId": 4
      },
      "lineNbr": 21,
      "author": {
        "id": 4393
      },
      "writtenOn": "2020-06-24T13:35:23Z",
      "side": 1,
      "message": "It\u0027s going to have to be used by nova\u0027s multistore ceph job at least. I\u0027ve never added a Needed-By link, but.. sure.",
      "parentUuid": "bf51134e_4ad8f11c",
      "revId": "f308e76d6547b57a7d66bb7d1eb64337c6e8cac0",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "bf51134e_6ae335da",
        "filename": "/COMMIT_MSG",
        "patchSetId": 4
      },
      "lineNbr": 25,
      "author": {
        "id": 13252
      },
      "writtenOn": "2020-06-24T06:33:01Z",
      "side": 1,
      "message": "This doesn\u0027t work as we consume OSC from pypi it seems.",
      "revId": "f308e76d6547b57a7d66bb7d1eb64337c6e8cac0",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "bf51134e_56d89379",
        "filename": "/COMMIT_MSG",
        "patchSetId": 4
      },
      "lineNbr": 25,
      "author": {
        "id": 4393
      },
      "writtenOn": "2020-06-24T13:35:23Z",
      "side": 1,
      "message": "I was depends-on\u0027ing this from a job that configures osc to be built from git. I just got in and haven\u0027t checked that zuul kept that dep all the way through here, but that was the intention.",
      "parentUuid": "bf51134e_6ae335da",
      "revId": "f308e76d6547b57a7d66bb7d1eb64337c6e8cac0",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "bf51134e_4abd9100",
        "filename": "functions",
        "patchSetId": 4
      },
      "lineNbr": 107,
      "author": {
        "id": 13252
      },
      "writtenOn": "2020-06-24T06:33:01Z",
      "side": 1,
      "message": "Just being curious: Is there a technical reason for this or is this just a bug in glance that prevents this?",
      "revId": "f308e76d6547b57a7d66bb7d1eb64337c6e8cac0",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "bf51134e_16b3bb4e",
        "filename": "functions",
        "patchSetId": 4
      },
      "lineNbr": 107,
      "author": {
        "id": 4393
      },
      "writtenOn": "2020-06-24T13:35:23Z",
      "side": 1,
      "message": "Yeah, glance is unable to spawn background threads that run to completion when used under uwsgi. Nova has the same problem, which prevents checking things like rabbit heartbeats, which we\u0027ve had to squelch since the move to uwsgi.\n\nSince things like import format conversion, metadata tagging, and import copy-to-store all require background threads in the glance-api worker, these features will never work under uwsgi.\n\nTBH, my next step after getting all this stuff working and tested with WSGI_MODE\u003dmod_wsgi, was going to be to suggest a way to make glance-api never run under uwsgi, since it pretty much can\u0027t function normally at all in that way. At RedHat, we don\u0027t run it under uwsgi in the product that way, and testing it like that makes this not very realistic.",
      "parentUuid": "bf51134e_4abd9100",
      "revId": "f308e76d6547b57a7d66bb7d1eb64337c6e8cac0",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "bf51134e_f1fa9172",
        "filename": "functions",
        "patchSetId": 4
      },
      "lineNbr": 107,
      "author": {
        "id": 13252
      },
      "writtenOn": "2020-06-24T14:29:37Z",
      "side": 1,
      "message": "IIUC neutron has created a neutron-rpc service to be handling such things when neutron-server is running under uwsgi.\n\nBut indeed I would like to consider devstack not only a testing tool, but also a blueprint for how to deploy OpenStack in the real world, so I\u0027m all for making stuff more realistic.",
      "parentUuid": "bf51134e_16b3bb4e",
      "revId": "f308e76d6547b57a7d66bb7d1eb64337c6e8cac0",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "bf51134e_9116b554",
        "filename": "functions",
        "patchSetId": 4
      },
      "lineNbr": 107,
      "author": {
        "id": 4393
      },
      "writtenOn": "2020-06-24T14:44:00Z",
      "side": 1,
      "message": "Yep, and Nova has workers over RPC to handle such things (other than what can\u0027t be delegated like the heartbeat checks). Since glance needs to stream the image it\u0027s working on, it\u0027s kinda fundamentally not able to delegate long-running tasks like this, as it wouldn\u0027t be reasonable to either stream a multi-GiB image over RPC, nor require a shared highly-available filesystem or similar. Obviously addressing this would be a fundamental architectural change to Glance, which just isn\u0027t feasible, especially given they are already struggling to keep up with maintenance.",
      "parentUuid": "bf51134e_f1fa9172",
      "revId": "f308e76d6547b57a7d66bb7d1eb64337c6e8cac0",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "bf51134e_6a34d593",
        "filename": "functions",
        "patchSetId": 4
      },
      "lineNbr": 406,
      "author": {
        "id": 13252
      },
      "writtenOn": "2020-06-24T06:33:01Z",
      "side": 1,
      "message": "img_property, same below",
      "range": {
        "startLine": 406,
        "startChar": 97,
        "endLine": 406,
        "endChar": 111
      },
      "revId": "f308e76d6547b57a7d66bb7d1eb64337c6e8cac0",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "bf51134e_0a239957",
        "filename": "functions",
        "patchSetId": 4
      },
      "lineNbr": 433,
      "author": {
        "id": 13252
      },
      "writtenOn": "2020-06-24T06:33:01Z",
      "side": 1,
      "message": "Insert $img_property here, too?",
      "revId": "f308e76d6547b57a7d66bb7d1eb64337c6e8cac0",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "bf51134e_cab36109",
        "filename": "lib/tempest",
        "patchSetId": 4
      },
      "lineNbr": 157,
      "author": {
        "id": 13252
      },
      "writtenOn": "2020-06-24T06:33:01Z",
      "side": 1,
      "message": "Can you please add a retry counter with a limit? Waiting forever without a timeout when somethings gets stuck should be avoided.\n\nI\u0027m also unsure whether this wait should actually happen here or directly after the image upload in lib/glance, which would also protect other possible consumers.",
      "revId": "f308e76d6547b57a7d66bb7d1eb64337c6e8cac0",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "bf51134e_36e4ff3b",
        "filename": "lib/tempest",
        "patchSetId": 4
      },
      "lineNbr": 157,
      "author": {
        "id": 4393
      },
      "writtenOn": "2020-06-24T13:35:23Z",
      "side": 1,
      "message": "Waiting here makes the job take less time because we overlap the import operation with more stack setup. Only here does it matter, and if you were to not enable tempest, it wouldn\u0027t matter if it didn\u0027t finish by the time devstack ended.\n\nYes, I can add a limit. This was to catch an error so we could see it and I relied on the job timeout to eventually kill it.",
      "parentUuid": "bf51134e_cab36109",
      "revId": "f308e76d6547b57a7d66bb7d1eb64337c6e8cac0",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "bf51134e_11a745a9",
        "filename": "lib/tempest",
        "patchSetId": 4
      },
      "lineNbr": 157,
      "author": {
        "id": 13252
      },
      "writtenOn": "2020-06-24T14:29:37Z",
      "side": 1,
      "message": "\u003e Waiting here makes the job take less time because we overlap the\n \u003e import operation with more stack setup. Only here does it matter,\n \u003e and if you were to not enable tempest, it wouldn\u0027t matter if it\n \u003e didn\u0027t finish by the time devstack ended.\n\nWhat about e.g. grenade? That will run devstack to install the \"old\" side and then start the upgrade procedure. What will happen to the image if glance api is restarted halfway?\n\n \u003e Yes, I can add a limit. This was to catch an error so we could see\n \u003e it and I relied on the job timeout to eventually kill it.\n\nIf we can call this failed after like 5 minutes, we don\u0027t need to wait two more hours. Also devstack isn\u0027t only used for jobs but also by users to deploy a developing environment.",
      "parentUuid": "bf51134e_36e4ff3b",
      "revId": "f308e76d6547b57a7d66bb7d1eb64337c6e8cac0",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "bf51134e_ec13da5a",
        "filename": "lib/tempest",
        "patchSetId": 4
      },
      "lineNbr": 157,
      "author": {
        "id": 4393
      },
      "writtenOn": "2020-06-24T14:44:00Z",
      "side": 1,
      "message": "Yep, understand, no argument about a timeout. I put this here because the image was _never_ going into active state and the below search for active images wasn\u0027t showing us useful info. I think that a specific check that something went to active is a good idea, which is why I left it here, but am happy to make it timeout.\n\nTBH, I\u0027m not super concerned about the grenade case as I don\u0027t think there\u0027s any reason to need to configure a grenade job to use the import workflow in the short term (we don\u0027t test ceph deployments with grenade currently, AFAIK). In reality, if things don\u0027t break, a cirros image should upload and convert very fast and not ever even need us to wait here.\n\nBut, if you would rather me put the wait loop near image upload for the future, then that\u0027s fine.",
      "parentUuid": "bf51134e_11a745a9",
      "revId": "f308e76d6547b57a7d66bb7d1eb64337c6e8cac0",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    }
  ]
}